'''
Script for the validation of GAFs produced from the reads generated by this crate.

USAGE:
First, generate some reads using this crate, i.e.:
    handlegraph-utils -i graph.gfa -n 1000 --len 100 --out reads.fa

Then, align them with your favorite aligner and obtain a GAF. For example, we can use GraphAligner:
    GraphAligner -g graph.gfa -f reads.fa -a alignments.gaf -x vg

Install ODGI and follow the guide to get the Python bindings working. Then create a .og file from the GFA:
    odgi build -g graph.gfa -o graph.og

Finally, call this script on the resulting GAF. One of the required parameters is the treshold to be used in order
for the alignment to be considered "correct".
    python3 validateGAF.py alignments.gaf 0.5 graph.og reads.fa

Q&A:
Q - Can I use this script with reads generated by other tools other than handlegraph-utils?
A - NO, it needs additional information in the header of each read for it to work properly

Q - What aligners can be used to obtained the GAF?
A - GraphAligner, GraphChainer and rs-vgaligner all work with this script.
    VG map may not work because the GAF is slightly different (this can be fixed by editing the parseGAF function).
'''

import argparse
import pandas as pd
import re
import json
import sys
import odgi
# Normalized because we want a similarity between 0 and 1.
# Non-normalized would be the actual number of single-character edits.
from strsimpy.normalized_levenshtein import NormalizedLevenshtein
normalized_levenshtein = NormalizedLevenshtein()

# Parse GAF to a dataframe
def parse_GAF(path_to_file):
    gaf_fields = ["name", "qlen", "qstart", "qend",
                  "strand", "path", "plen", "pstart", "pend",
                  "residue", "alblock", "quality", "extra",
                  "extra1", "extra2", "extra3", "extra4"]
    my_gaf = pd.read_csv(path_to_file, sep='\t', names=gaf_fields)
    return my_gaf

# Parse FASTA
class FASTARecord:
    def __init__(self, name, seq):
        self.name = name
        self.seq = seq

def parse_FASTA(path_to_file):
    records = []
    with open(path_to_file) as f:
        for ext_name, seq in zip(f, f):
            name = ext_name.split(" {")[0][1:]
            records.append(FASTARecord(name, seq))
    return records


# CLI arguments parsing
parser = argparse.ArgumentParser(description='Returns the accuracy of a gaf GAF')
parser.add_argument('GAF', help='Path to the GAF file')
parser.add_argument("Threshold", help="Threshold that should be used to consider a read mapped correctly")
parser.add_argument('Graph', help='Path to graph in .odgi/.og format')
parser.add_argument('Reads', help='Path to reads in .fasta/.fa format (generated by handlegraph-utils!)')
args = vars(parser.parse_args())

print("Command used: {}".format(" ".join(sys.argv)))
print("GAF file: {}".format(args["GAF"]))

# Read GAF file
my_gaf = parse_GAF(args["GAF"])

# Read FA file
my_reads = parse_FASTA(args["Reads"])

# Print out some debug information
threshold = float(args["Threshold"])
print("Threshold is: {}\n".format(threshold))

# Load ODGI graph
g = odgi.graph()
g.load(args["Graph"])

count_correct = 0

# For each GAF record
for i in range(len(my_gaf)):

    ##### Obtain truth values (node ids, offsets, errors)
    my_gaf_row = my_gaf.iloc[i]
    my_gaf_name = my_gaf_row["name"]

    #TODO: improve this, since jsonarrays can contain " ", split is not what I should do (maybe try REs?)
    name = my_gaf_row["name"].split(" {")[0]
    metadata = ' {'+my_gaf_row["name"].split(" {")[1]
    read_gen_data = json.loads(metadata)

    #print(read_gen_data)
    truth_nodes = [int(node) for node in read_gen_data["nodes"]]
    start_offset = read_gen_data["start_offset"]
    end_offset = read_gen_data["end_offset"]
    if "errors" in read_gen_data:
        errors = read_gen_data["errors"]

    ##### Get sequence from read
    curr_read = my_reads[i]
    curr_read_seq = curr_read.seq
    qstart = my_gaf_row["qstart"]
    qend = my_gaf_row["qend"]
    curr_read_aln = curr_read_seq[qstart:qend]


    ##### Get sequence from truth
    truth_as_string = ""
    truth_as_list = []
    for pos,nodeid in enumerate(truth_nodes):
        handle = g.get_handle(nodeid)
        handle_seq = g.get_sequence(handle)
        if pos == 0:
            handle_seq = handle_seq[start_offset:]
        if pos == len(truth_nodes) - 1:
            handle_seq = handle_seq[:end_offset]
        truth_as_list.append(handle_seq)
    truth_as_string = "".join(truth_as_list)

    ##### Obtain alignment values
    my_gaf_nodes_str = my_gaf_row["path"]
    aln_start = my_gaf_row["pstart"]
    aln_end = my_gaf_row["pend"]

    if my_gaf_nodes_str == '*':
        continue

    my_gaf_tuples = re.findall("(>|<)([0-9]+)", my_gaf_nodes_str)
    my_gaf_int = list(map(lambda x: int(x[1]), my_gaf_tuples))

    ##### Get sequence from aln
    aln_as_string = ""
    aln_as_list = []
    for pos,nodeid in enumerate(my_gaf_int):
        handle = g.get_handle(nodeid)
        handle_seq = g.get_sequence(handle)
        if pos == 0:
            handle_seq = handle_seq[start_offset:]
        if pos == len(truth_nodes) - 1:
            handle_seq = handle_seq[:end_offset]
        aln_as_list.append(handle_seq)
    aln_as_string = "".join(aln_as_list)

    ##### Return edit distance and check simple equality
    #edit_similarity = normalized_levenshtein.similarity(truth_as_string, aln_as_string)
    edit_similarity = normalized_levenshtein.similarity(curr_read_aln, aln_as_string)
    print(curr_read.name, name)
    assert(curr_read.name == name)

    if edit_similarity >= threshold:
        count_correct += 1
    else:
        print(curr_read_seq)
        print(aln_as_string)
        if "errors" in read_gen_data:
            print("Edit similarity for {} is {} (errors: {})".format(name, edit_similarity, errors))
        else:
            print("Edit distance for {} is {}".format(name, 1-edit_similarity))
        print("TRUTH nodes are:", truth_nodes)
        print("ALN nodes are: {}".format(my_gaf_int))
        #print("TRUTH seqs are: {}".format(truth_as_list))
        #print("ALN seqs are: {}".format(aln_as_list))

print("There were {}/{} correct reads".format(count_correct, len(my_gaf)))